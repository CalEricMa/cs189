{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "import random\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "eps = 1e-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        # TODO\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        p = counts/counts.sum()\n",
    "\n",
    "        return -np.sum(p * np.log2(p + eps)) #add eps to avoid log(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        # TODO\n",
    "        cur_entropy = DecisionTree.entropy(y)\n",
    "        left = X < thresh\n",
    "        right = ~left\n",
    "        y_left = y[left]\n",
    "        y_right = y[right]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            return 0\n",
    "        \n",
    "        left_entropy = DecisionTree.entropy(y_left)\n",
    "        right_entropy = DecisionTree.entropy(y_right)\n",
    "\n",
    "        weighted_entropy = (len(y_left) / len(y)) * left_entropy + (len(y_right) / len(y)) * right_entropy\n",
    "\n",
    "        return cur_entropy - weighted_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        # OPTIONAL\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        # OPTIONAL\n",
    "        pass\n",
    "\n",
    "    def split(self, X, y, feature_idx, thresh):\n",
    "        \"\"\"\n",
    "        Split the dataset into two subsets, given a feature and a threshold.\n",
    "        Return X_0, y_0, X_1, y_1\n",
    "        where (X_0, y_0) are the subset of examples whose feature_idx-th feature\n",
    "        is less than thresh, and (X_1, y_1) are the other examples.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        left_mask = X[:, feature_idx] < thresh\n",
    "        right_mask = ~left_mask \n",
    "        X_left = X[left_mask]\n",
    "        y_left = y[left_mask]\n",
    "        X_right = X[right_mask]\n",
    "        y_right = y[right_mask]\n",
    "        return X_left, y_left, X_right, y_right\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        self.labels = y\n",
    "\n",
    "        if self.max_depth == 0 or len(np.unique(y)) == 1:\n",
    "            self.pred = Counter(y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        best_info_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "        best_splits = None\n",
    "\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "\n",
    "            values = np.unique(X[:, i])\n",
    "            if len(values) <= 1:\n",
    "                continue  \n",
    "            for i in range(len(values) - 1):\n",
    "                thresh = (values[i] + values[i+1]) / 2.0\n",
    "                ig = DecisionTree.information_gain(X[:, i], y, thresh)\n",
    "                if ig > best_info_gain:\n",
    "                    best_info_gain = ig\n",
    "                    best_feature = i\n",
    "                    best_thresh = thresh\n",
    "                    best_splits = self.split(X, y, i, thresh)\n",
    "\n",
    "        if best_info_gain <= 0 or best_splits is None:\n",
    "            self.pred = Counter(y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        self.split_idx = best_feature\n",
    "        self.thresh = best_thresh\n",
    "\n",
    "        X_left, y_left, X_right, y_right = best_splits\n",
    "        self.left = DecisionTree(max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "        self.left.fit(X_left, y_left)\n",
    "        self.right = DecisionTree(max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "        self.right.fit(X_right, y_right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        predictions = [self._predict_row(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict_row(self, x):\n",
    "        if self.pred is not None:\n",
    "            return self.pred\n",
    "        if x[self.split_idx] < self.thresh:\n",
    "            return self.left._predict_row(x)\n",
    "        else:\n",
    "            return self.right._predict_row(x)\n",
    "    \n",
    "\n",
    "    def _to_graphviz(self, node_id):\n",
    "        if self.max_depth == 0:\n",
    "            return f'{node_id} [label=\"Prediction: {self.pred}\\nSamples: {self.labels.size}\"];\\n'\n",
    "        else:\n",
    "            graph = f'{node_id} [label=\"{self.features[self.split_idx]} < {self.thresh:.2f}\"];\\n'\n",
    "            left_id = node_id * 2 + 1\n",
    "            right_id = node_id * 2 + 2\n",
    "            if self.left is not None:\n",
    "                graph += f'{node_id} -> {left_id};\\n'\n",
    "                graph += self.left._to_graphviz(left_id)\n",
    "            if self.right is not None:\n",
    "                graph += f'{node_id} -> {right_id};\\n'\n",
    "                graph += self.right._to_graphviz(right_id)\n",
    "            return graph\n",
    "\n",
    "    def to_graphviz(self):\n",
    "        graph = \"digraph Tree {\\nnode [shape=box];\\n\"\n",
    "        graph += self._to_graphviz(0)\n",
    "        graph += \"}\\n\"\n",
    "        return graph\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        n_samples = X.shape[0]\n",
    "        for tree in self.decision_trees:\n",
    "            bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_bootstrap = X[bootstrap_indices]\n",
    "            y_bootstrap = y[bootstrap_indices]\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        all_preds = np.array([tree.predict(X) for tree in self.decision_trees])\n",
    "        all_preds = all_preds.T  \n",
    "        majority_votes = []\n",
    "        for row in all_preds:\n",
    "            majority_votes.append(Counter(row).most_common(1)[0][0])\n",
    "        return np.array(majority_votes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['max_features'] = m\n",
    "        self.m = m\n",
    "        super().__init__(params=params, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == b''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == b'-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack(\n",
    "        [np.array(data, dtype=float),\n",
    "         np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "    return data, onehot_features\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        print(\"First splits\", first_splits)\n",
    "\n",
    "\n",
    "def generate_submission(testing_data, predictions, dataset=\"titanic\"):\n",
    "    assert dataset in [\"titanic\", \"spam\"], f\"dataset should be either 'titanic' or 'spam'\"\n",
    "    # This code below will generate the predictions.csv file.\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        predictions = predictions.astype(int)\n",
    "    else:\n",
    "        predictions = np.array(predictions, dtype=int)\n",
    "    assert predictions.shape == (len(testing_data),), \"Predictions were not the correct shape\"\n",
    "    df = pd.DataFrame({'Category': predictions})\n",
    "    df.index += 1  # Ensures that the index starts at 1.\n",
    "    df.to_csv(f'predictions_{dataset}.csv', index_label='Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (b): preprocessing the titanic dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: np.str_('')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y[labeled_idx], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPart (b): preprocessing the titanic dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m X, onehot_features \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monehot_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m X \u001b[38;5;241m=\u001b[39m X[labeled_idx, :]\n\u001b[0;32m     25\u001b[0m Z, _ \u001b[38;5;241m=\u001b[39m preprocess(test_data[\u001b[38;5;241m1\u001b[39m:, :], onehot_cols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m])\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(data, fill_mode, min_freq, onehot_cols)\u001b[0m\n\u001b[0;32m     17\u001b[0m     data[:, col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m onehot_encoding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(onehot_encoding)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     19\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(\n\u001b[1;32m---> 20\u001b[0m     [np\u001b[38;5;241m.\u001b[39marray(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m),\n\u001b[0;32m     21\u001b[0m      np\u001b[38;5;241m.\u001b[39marray(onehot_encoding)])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Replace missing data with the mode value. We use the mode instead of\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# the mean or median because this makes more sense for categorical\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# features such as gender or cabin type, which are not ordered.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill_mode:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: np.str_('')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = \"titanic\"\n",
    "    # dataset = \"spam\"\n",
    "    params = {\n",
    "        \"max_depth\": 5,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "    N = 100\n",
    "\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = 'datasets/titanic/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "        path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "        test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "        y = data[1:, 0]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "        labeled_idx = np.delete(np.arange(len(y)), 705)\n",
    "        y = np.array(y[labeled_idx], dtype=float).astype(int)\n",
    "        print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "        X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "        X = X[labeled_idx, :]\n",
    "        Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "        assert X.shape[1] == Z.shape[1]\n",
    "        features = list(data[0, 1:]) + onehot_features\n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = 'datasets/spam_data/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "    print(\"Features\", features)\n",
    "    print(\"Train/test size\", X.shape, Z.shape)\n",
    "\n",
    "    # Decision Tree\n",
    "    print(\"\\n\\nDecision Tree\")\n",
    "    dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "    dt.fit(X, y)\n",
    "\n",
    "    # Visualize Decision Tree\n",
    "    print(\"\\n\\nTree Structure\")\n",
    "    # Print using repr\n",
    "    print(dt.__repr__())\n",
    "    # Save tree to pdf\n",
    "    graph_from_dot_data(dt.to_graphviz())[0].write_pdf(\"%s-basic-tree.pdf\" % dataset)\n",
    "\n",
    "    # Random Forest\n",
    "    print(\"\\n\\nRandom Forest\")\n",
    "    rf = RandomForest(params, n=N, m=np.int_(np.sqrt(X.shape[1])))\n",
    "    rf.fit(X, y)\n",
    "    evaluate(rf)\n",
    "\n",
    "    # Generate Test Predictions\n",
    "    print(\"\\n\\nGenerate Test Predictions\")\n",
    "    pred = rf.predict(Z)\n",
    "    generate_submission(Z, pred, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
