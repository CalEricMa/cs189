{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To prepare the starter code, copy this file over to decision_tree_starter.py\n",
    "and go through and handle all the inline TODOs.\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "\n",
    "import random\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "eps = 1e-5  # a small number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        # TODO\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        p = counts/counts.sum()\n",
    "\n",
    "        return -np.sum(p * np.log2(p + eps)) #add eps to avoid log(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        # TODO\n",
    "        cur_entropy = DecisionTree.entropy(y)\n",
    "        left = X < thresh\n",
    "        right = ~left\n",
    "        y_left = y[left]\n",
    "        y_right = y[right]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            return 0\n",
    "        \n",
    "        left_entropy = DecisionTree.entropy(y_left)\n",
    "        right_entropy = DecisionTree.entropy(y_right)\n",
    "\n",
    "        weighted_entropy = (len(y_left) / len(y)) * left_entropy + (len(y_right) / len(y)) * right_entropy\n",
    "\n",
    "        return cur_entropy - weighted_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        # OPTIONAL\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        # OPTIONAL\n",
    "        pass\n",
    "\n",
    "    def split(self, X, y, feature_idx, thresh):\n",
    "        \"\"\"\n",
    "        Split the dataset into two subsets, given a feature and a threshold.\n",
    "        Return X_0, y_0, X_1, y_1\n",
    "        where (X_0, y_0) are the subset of examples whose feature_idx-th feature\n",
    "        is less than thresh, and (X_1, y_1) are the other examples.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        left = X[:, feature_idx] < thresh\n",
    "        right = ~left \n",
    "        X_left = X[left]\n",
    "        y_left = y[left]\n",
    "        X_right = X[right]\n",
    "        y_right = y[right]\n",
    "        return X_left, y_left, X_right, y_right\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        self.labels = y\n",
    "\n",
    "        #if hit max depth or all same labels\n",
    "        if self.max_depth == 0 or len(np.unique(y)) == 1:\n",
    "            self.pred = Counter(y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        best_info_gain = -np.inf\n",
    "        best_feature_idx = None\n",
    "        best_thresh = None\n",
    "        best_splits = None\n",
    "\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "\n",
    "            values = np.unique(X[:, i])\n",
    "            if len(values) <= 1:\n",
    "                continue\n",
    "            for j in range(len(values) - 1): \n",
    "                thresh = (values[j] + values[j + 1]) / 2\n",
    "                curr_gain = DecisionTree.information_gain(X[:, i], y, thresh)\n",
    "                if curr_gain > best_info_gain:\n",
    "                    best_info_gain = curr_gain\n",
    "                    best_feature_idx = i\n",
    "                    best_thresh = thresh\n",
    "                    best_splits = self.split(X, y, i, thresh)\n",
    "\n",
    "        if best_info_gain <= 0 or best_splits is None:\n",
    "            self.pred = Counter(y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        self.split_idx = best_feature_idx\n",
    "        self.thresh = best_thresh\n",
    "\n",
    "        X_left, y_left, X_right, y_right = best_splits\n",
    "        self.left = DecisionTree(self.max_depth - 1, self.features)\n",
    "        self.left.fit(X_left, y_left)\n",
    "        self.right = DecisionTree(self.max_depth - 1, self.features)\n",
    "        self.right.fit(X_right, y_right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        res = []\n",
    "        for x in X:\n",
    "            node = self\n",
    "            while node.pred is None:\n",
    "                if x[node.split_idx] < node.thresh:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            res.append(node.pred)\n",
    "        return np.array(res)\n",
    "    \n",
    "    \n",
    "    def _to_graphviz(self, node_id):\n",
    "        # If this node is a leaf, indicated by self.pred being set, then just show the prediction.\n",
    "        if self.pred is not None:\n",
    "            return f'{node_id} [label=\"Prediction: {self.pred}\\nSamples: {len(self.labels)}\"];\\n'\n",
    "        else:\n",
    "            graph = f'{node_id} [label=\"{self.features[self.split_idx]} < {self.thresh:.2f}\"];\\n'\n",
    "            left_id = node_id * 2 + 1\n",
    "            right_id = node_id * 2 + 2\n",
    "            if self.left is not None:\n",
    "                graph += f'{node_id} -> {left_id};\\n'\n",
    "                graph += self.left._to_graphviz(left_id)\n",
    "            if self.right is not None:\n",
    "                graph += f'{node_id} -> {right_id};\\n'\n",
    "                graph += self.right._to_graphviz(right_id)\n",
    "            return graph\n",
    "\n",
    "\n",
    "    def to_graphviz(self):\n",
    "        graph = \"digraph Tree {\\nnode [shape=box];\\n\"\n",
    "        graph += self._to_graphviz(0)\n",
    "        graph += \"}\\n\"\n",
    "        return graph\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # If this node is a leaf, self.pred will be set.\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        if self.pred is not None:\n",
    "            return \"%s (%s)\" % (self.pred, len(self.labels))\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                        self.thresh, self.left.__repr__(),\n",
    "                                        self.right.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        n_samples = X.shape[0]\n",
    "        for tree in self.decision_trees:\n",
    "            bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_bootstrap = X[bootstrap_idx]\n",
    "            y_bootstrap = y[bootstrap_idx]\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        all_preds = np.array([tree.predict(X) for tree in self.decision_trees])\n",
    "        all_preds = all_preds.T  \n",
    "        votes = []\n",
    "        for row in all_preds:\n",
    "            votes.append(Counter(row).most_common(1)[0][0])\n",
    "        return np.array(votes)\n",
    "    \n",
    "\n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['max_features'] = m\n",
    "        self.m = m\n",
    "        super().__init__(params=params, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    data = np.array(data, dtype=str)\n",
    "    # Replace empty strings with the placeholder '-1'\n",
    "    data[data == ''] = '-1'\n",
    "    \n",
    "    # One-hot encode specified columns\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term, freq in counter.most_common():\n",
    "            if term == '-1': \n",
    "                continue\n",
    "            if freq <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term)\n",
    "            onehot_encoding.append((data[:, col] == term).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    \n",
    "    if len(onehot_encoding) > 0:\n",
    "        onehot_encoding = np.array(onehot_encoding).T\n",
    "    else:\n",
    "        onehot_encoding = np.empty((data.shape[0], 0))\n",
    "    data_numeric = np.array(data, dtype=float)\n",
    "    \n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        # TODO\n",
    "        for i in range(data_numeric.shape[1]):\n",
    "    \n",
    "            missing = np.abs(data_numeric[:, i] + 1) < eps\n",
    "            non_missing = data_numeric[~missing, i]\n",
    "            if non_missing.size == 0:\n",
    "                continue \n",
    "          \n",
    "            non_missing = np.array(non_missing)\n",
    "            \n",
    "            if non_missing.ndim == 0 or non_missing.size == 1:\n",
    "                mode_val = non_missing.item()\n",
    "            else:\n",
    "                counts = Counter(non_missing)\n",
    "                mode_val = counts.most_common(1)[0][0]\n",
    "\n",
    "            data_numeric[missing, i] = mode_val\n",
    "    \n",
    "    final_data = np.hstack([data_numeric, onehot_encoding])\n",
    "    \n",
    "    return final_data, onehot_features\n",
    "\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        print(\"First splits\", first_splits)\n",
    "\n",
    "\n",
    "def generate_submission(testing_data, predictions, dataset=\"titanic\"):\n",
    "    assert dataset in [\"titanic\", \"spam\"], f\"dataset should be either 'titanic' or 'spam'\"\n",
    "    # This code below will generate the predictions.csv file.\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        predictions = predictions.astype(int)\n",
    "    else:\n",
    "        predictions = np.array(predictions, dtype=int)\n",
    "    assert predictions.shape == (len(testing_data),), \"Predictions were not the correct shape\"\n",
    "    df = pd.DataFrame({'Category': predictions})\n",
    "    df.index += 1  # Ensures that the index starts at 1.\n",
    "    df.to_csv(f'predictions_{dataset}.csv', index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features ['pain', 'private', 'bank', 'money', 'drug', 'spam', 'prescription', 'creative', 'height', 'featured', 'differ', 'width', 'other', 'energy', 'business', 'message', 'volumes', 'revision', 'path', 'meter', 'memo', 'planning', 'pleased', 'record', 'out', 'semicolon', 'dollar', 'sharp', 'exclamation', 'parenthesis', 'square_bracket', 'ampersand']\n",
      "Train/test size (5172, 32) (5857, 32)\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "\n",
      "\n",
      "Tree Structure\n",
      "[exclamation < 0.5: [meter < 0.5: [parenthesis < 0.5: 0 (2060) | 0 (1169)] | 0 (688)] | [meter < 0.5: [ampersand < 0.5: 1 (1043) | 0 (127)] | 0 (85)]]\n",
      "\n",
      "\n",
      "Random Forest\n",
      "Cross validation [0.79323671 0.82898551 0.81237911 0.7901354  0.80464217]\n",
      "First splits [('money', 16), ('meter', 14), ('exclamation', 13), ('pain', 8), ('volumes', 7), ('featured', 7), ('spam', 6), ('dollar', 6), ('parenthesis', 6), ('ampersand', 5), ('prescription', 4), ('sharp', 2), ('creative', 2), ('private', 1), ('differ', 1), ('revision', 1), ('business', 1)]\n",
      "max_depth: 3, min_samples_leaf: 5, CV Score: 0.7589\n",
      "max_depth: 3, min_samples_leaf: 10, CV Score: 0.7622\n",
      "max_depth: 3, min_samples_leaf: 15, CV Score: 0.7624\n",
      "max_depth: 5, min_samples_leaf: 5, CV Score: 0.8123\n",
      "max_depth: 5, min_samples_leaf: 10, CV Score: 0.8010\n",
      "max_depth: 5, min_samples_leaf: 15, CV Score: 0.7991\n",
      "max_depth: 10, min_samples_leaf: 5, CV Score: 0.8215\n",
      "max_depth: 10, min_samples_leaf: 10, CV Score: 0.8157\n",
      "max_depth: 10, min_samples_leaf: 15, CV Score: 0.8142\n",
      "max_depth: 15, min_samples_leaf: 5, CV Score: 0.8231\n",
      "max_depth: 15, min_samples_leaf: 10, CV Score: 0.8192\n",
      "max_depth: 15, min_samples_leaf: 15, CV Score: 0.8126\n",
      "max_depth: 20, min_samples_leaf: 5, CV Score: 0.8244\n",
      "max_depth: 20, min_samples_leaf: 10, CV Score: 0.8140\n",
      "max_depth: 20, min_samples_leaf: 15, CV Score: 0.8105\n",
      "max_depth: 30, min_samples_leaf: 5, CV Score: 0.8239\n",
      "max_depth: 30, min_samples_leaf: 10, CV Score: 0.8186\n",
      "max_depth: 30, min_samples_leaf: 15, CV Score: 0.8152\n",
      "\n",
      "Best Hyperparameters:\n",
      "max_depth: 20, min_samples_leaf: 5, with CV Score: 0.8244\n"
     ]
    }
   ],
   "source": [
    "dataset = \"spam\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 100\n",
    "\n",
    "if dataset == \"spam\":\n",
    "    features = [\n",
    "        \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "        \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "        \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "        \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "        \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "        \"square_bracket\", \"ampersand\"\n",
    "    ]\n",
    "    assert len(features) == 32\n",
    "\n",
    "    # Load spam data from the .mat file\n",
    "    path_train = 'datasets/spam_data/spam_data.mat'\n",
    "    data = scipy.io.loadmat(path_train)\n",
    "    X = data['training_data']\n",
    "    y = np.squeeze(data['training_labels'])\n",
    "    Z = data['test_data']\n",
    "    class_names = [\"Ham\", \"Spam\"]\n",
    "else:\n",
    "    raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "print(\"Features\", features)\n",
    "print(\"Train/test size\", X.shape, Z.shape)\n",
    "\n",
    "# Decision Tree (for visualization/debugging)\n",
    "print(\"\\n\\nDecision Tree\")\n",
    "dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt.fit(X, y)\n",
    "print(\"\\n\\nTree Structure\")\n",
    "print(dt.__repr__())\n",
    "graph_from_dot_data(dt.to_graphviz())[0].write_pdf(\"%s-basic-tree.pdf\" % dataset)\n",
    "\n",
    "# Random Forest (final model)\n",
    "print(\"\\n\\nRandom Forest\")\n",
    "rf = RandomForest(params, n=N, m=int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "evaluate(rf)\n",
    "\n",
    "# Hyperparameter tuning using a simple for-loop over max_depth and min_samples_leaf:\n",
    "max_depth_candidates = [3, 5, 10, 15, 20, 30]\n",
    "min_samples_leaf_candidates = [5, 10, 15]\n",
    "\n",
    "best_cv_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for max_depth in max_depth_candidates:\n",
    "    for min_samples_leaf in min_samples_leaf_candidates:\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_samples_leaf'] = min_samples_leaf\n",
    "        rf_temp = RandomForest(params=params, n=N, m=int(np.sqrt(X.shape[1])))\n",
    "        rf_temp.fit(X, y)\n",
    "        cv_scores = cross_val_score(rf_temp, X, y, cv=5)\n",
    "        avg_cv_score = np.mean(cv_scores)\n",
    "        \n",
    "        print(f\"max_depth: {max_depth}, min_samples_leaf: {min_samples_leaf}, CV Score: {avg_cv_score:.4f}\")\n",
    "        \n",
    "        if avg_cv_score > best_cv_score:\n",
    "            best_cv_score = avg_cv_score\n",
    "            best_params = (max_depth, min_samples_leaf)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"max_depth: {best_params[0]}, min_samples_leaf: {best_params[1]}, with CV Score: {best_cv_score:.4f}\")\n",
    "\n",
    "rf_best = RandomForest(params={'max_depth': best_params[0], 'min_samples_leaf': best_params[1]}, n=N, m=int(np.sqrt(X.shape[1])))\n",
    "rf_best.fit(X, y)\n",
    "pred = rf_best.predict(Z)\n",
    "generate_submission(Z, pred, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Part (b): preprocessing the titanic dataset\n",
      "Features [np.str_('pclass'), np.str_('sex'), np.str_('age'), np.str_('sibsp'), np.str_('parch'), np.str_('ticket'), np.str_('fare'), np.str_('cabin'), np.str_('embarked'), np.str_('male'), np.str_('female'), np.str_('S'), np.str_('C'), np.str_('Q')]\n",
      "Train/test size (999, 14) (310, 14)\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "\n",
      "\n",
      "Tree Structure\n",
      "[male < 0.5: [pclass < 2.5: [S < 0.5: 1 (64) | 1 (125)] | [fare < 23.35: 1 (140) | 0 (20)]] | [age < 9.5: [sibsp < 2.0: 1 (20) | 0 (12)] | [pclass < 1.5: 0 (140) | 0 (478)]]]\n",
      "\n",
      "\n",
      "Random Forest\n",
      "Cross validation [0.805      0.815      0.77       0.835      0.79396985]\n",
      "First splits [(np.str_('male'), 27), (np.str_('fare'), 14), (np.str_('pclass'), 14), (np.str_('female'), 14), (np.str_('S'), 8), (np.str_('parch'), 8), (np.str_('age'), 7), (np.str_('C'), 4), (np.str_('sibsp'), 3), (np.str_('Q'), 1)]\n",
      "max_depth: 3, min_samples_leaf: 5, CV Score: 0.7858\n",
      "max_depth: 3, min_samples_leaf: 10, CV Score: 0.7918\n",
      "max_depth: 3, min_samples_leaf: 15, CV Score: 0.7918\n",
      "max_depth: 5, min_samples_leaf: 5, CV Score: 0.8038\n",
      "max_depth: 5, min_samples_leaf: 10, CV Score: 0.7958\n",
      "max_depth: 5, min_samples_leaf: 15, CV Score: 0.7938\n",
      "max_depth: 10, min_samples_leaf: 5, CV Score: 0.8038\n",
      "max_depth: 10, min_samples_leaf: 10, CV Score: 0.7958\n",
      "max_depth: 10, min_samples_leaf: 15, CV Score: 0.7858\n",
      "max_depth: 15, min_samples_leaf: 5, CV Score: 0.8058\n",
      "max_depth: 15, min_samples_leaf: 10, CV Score: 0.7978\n",
      "max_depth: 15, min_samples_leaf: 15, CV Score: 0.7918\n",
      "max_depth: 20, min_samples_leaf: 5, CV Score: 0.7978\n",
      "max_depth: 20, min_samples_leaf: 10, CV Score: 0.8028\n",
      "max_depth: 20, min_samples_leaf: 15, CV Score: 0.8018\n",
      "max_depth: 30, min_samples_leaf: 5, CV Score: 0.8028\n",
      "max_depth: 30, min_samples_leaf: 10, CV Score: 0.7978\n",
      "max_depth: 30, min_samples_leaf: 15, CV Score: 0.7838\n",
      "\n",
      "Best Hyperparameters:\n",
      "max_depth: 15, min_samples_leaf: 5, with CV Score: 0.8058\n"
     ]
    }
   ],
   "source": [
    "#do the same with titanic\n",
    "dataset = \"titanic\"\n",
    "# dataset = \"spam\"\n",
    "params = {\n",
    "    \"max_depth\": 5,\n",
    "    # \"random_state\": 6,\n",
    "    \"min_samples_leaf\": 10,\n",
    "}\n",
    "N = 100\n",
    "\n",
    "if dataset == \"titanic\":\n",
    "    # Load titanic data\n",
    "    path_train = 'datasets/titanic/titanic_training.csv'\n",
    "    data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "    path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "    test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "    y = data[1:, 0]  # label = survived\n",
    "    class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "    labeled_idx = np.delete(np.arange(len(y)), 705)\n",
    "    y = np.array(y[labeled_idx], dtype=float).astype(int)\n",
    "    print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "    X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "    X = X[labeled_idx, :]\n",
    "    Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "    assert X.shape[1] == Z.shape[1]\n",
    "    features = list(data[0, 1:]) + onehot_features\n",
    "\n",
    "elif dataset == \"spam\":\n",
    "    features = [\n",
    "        \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "        \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "        \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "        \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "        \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "        \"square_bracket\", \"ampersand\"\n",
    "    ]\n",
    "    assert len(features) == 32\n",
    "\n",
    "    # Load spam data\n",
    "    path_train = 'datasets/spam_data/spam_data.mat'\n",
    "    data = scipy.io.loadmat(path_train)\n",
    "    X = data['training_data']\n",
    "    y = np.squeeze(data['training_labels'])\n",
    "    Z = data['test_data']\n",
    "    class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "print(\"Features\", features)\n",
    "print(\"Train/test size\", X.shape, Z.shape)\n",
    "\n",
    "# Decision Tree (for visualization/debugging)\n",
    "print(\"\\n\\nDecision Tree\")\n",
    "dt = DecisionTree(max_depth=3, feature_labels=features)\n",
    "dt.fit(X, y)\n",
    "print(\"\\n\\nTree Structure\")\n",
    "print(dt.__repr__())\n",
    "graph_from_dot_data(dt.to_graphviz())[0].write_pdf(\"%s-basic-tree.pdf\" % dataset)\n",
    "\n",
    "# Random Forest (final model)\n",
    "print(\"\\n\\nRandom Forest\")\n",
    "rf = RandomForest(params, n=N, m=int(np.sqrt(X.shape[1])))\n",
    "rf.fit(X, y)\n",
    "evaluate(rf)\n",
    "\n",
    "# Hyperparameter tuning using a simple for-loop over max_depth and min_samples_leaf:\n",
    "max_depth_candidates = [3, 5, 10, 15, 20, 30]\n",
    "min_samples_leaf_candidates = [5, 10, 15]\n",
    "\n",
    "best_cv_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for max_depth in max_depth_candidates:\n",
    "    for min_samples_leaf in min_samples_leaf_candidates:\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_samples_leaf'] = min_samples_leaf\n",
    "        rf_temp = RandomForest(params=params, n=N, m=int(np.sqrt(X.shape[1])))\n",
    "        rf_temp.fit(X, y)\n",
    "        cv_scores = cross_val_score(rf_temp, X, y, cv=5)\n",
    "        avg_cv_score = np.mean(cv_scores)\n",
    "        \n",
    "        print(f\"max_depth: {max_depth}, min_samples_leaf: {min_samples_leaf}, CV Score: {avg_cv_score:.4f}\")\n",
    "        \n",
    "        if avg_cv_score > best_cv_score:\n",
    "            best_cv_score = avg_cv_score\n",
    "            best_params = (max_depth, min_samples_leaf)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"max_depth: {best_params[0]}, min_samples_leaf: {best_params[1]}, with CV Score: {best_cv_score:.4f}\")\n",
    "\n",
    "rf_best = RandomForest(params={'max_depth': best_params[0], 'min_samples_leaf': best_params[1]}, n=N, m=int(np.sqrt(X.shape[1])))\n",
    "rf_best.fit(X, y)\n",
    "pred = rf_best.predict(Z)\n",
    "generate_submission(Z, pred, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
